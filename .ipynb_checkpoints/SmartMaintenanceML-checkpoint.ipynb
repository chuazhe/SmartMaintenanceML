{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Purpose: Convert text file into readable csv file\n",
    "\n",
    "- Regression models: How many more cycles an engine will function before it fails?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\SmartMaintenanceML'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3','s4', 's5', 's6', 's7', 's8',\n",
    "         's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "# read training data\n",
    "train_data = pd.read_csv('Dataset/train_FD001.txt', sep=\" \", header=None)\n",
    "train_data.drop(train_data.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_data.columns = names\n",
    "\n",
    "train_data = train_data.sort_values(['id','cycle'])\n",
    "\n",
    "# read test data\n",
    "test_data = pd.read_csv('Dataset/test_FD001.txt', sep=\" \", header=None)\n",
    "test_data.drop(test_data.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_data.columns = names\n",
    "\n",
    "# read ground truth data\n",
    "truth_df = pd.read_csv('Dataset/RUL_FD001.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the size of the train dataset: 20631 entries and 26 features\n",
      "This is the size of the test dataset: 13096 entries and 26 features\n",
      "This is the size of the truth dataset: 100 entries and 1 features\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the size of the train dataset: {} entries and {} features\".format(train_data.shape[0], \n",
    "                                                                                 train_data.shape[1]))\n",
    "print(\"This is the size of the test dataset: {} entries and {} features\".format(test_data.shape[0],\n",
    "                                                                                test_data.shape[1]))\n",
    "print(\"This is the size of the truth dataset: {} entries and {} features\".format(truth_df.shape[0],\n",
    "                                                                                 truth_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 100 turbines in each dataset\n"
     ]
    }
   ],
   "source": [
    "n_turb = train_data[\"id\"].unique().max()\n",
    "n_train, n_features = train_data.shape\n",
    "print(\"There is {} turbines in each dataset\".format(n_turb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Labeling - generate column RUL\n",
    "rul = pd.DataFrame(train_data.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "train_data = train_data.merge(rul, on=['id'], how='left')\n",
    "train_data['RUL'] = train_data['max'] - train_data['cycle']\n",
    "train_data.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns\n",
    "# when rul=30, label1=1\n",
    "w1 = 30\n",
    "train_data['label1'] = np.where(train_data['RUL'] <= w1, 1, 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train data set is now: 20631 entries and 29 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data saved as Dataset/train.csv\n"
     ]
    }
   ],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "train_data['cycle_norm'] = train_data['cycle']\n",
    "cols_normalize = train_data.columns.difference(['id','cycle','RUL','label1'])\n",
    "min_max_scaler = MinMaxScaler()\n",
    "norm_train_data = pd.DataFrame(min_max_scaler.fit_transform(train_data[cols_normalize]),\n",
    "                               columns=cols_normalize, index=train_data.index)\n",
    "join_data = train_data[train_data.columns.difference(cols_normalize)].join(norm_train_data)\n",
    "train_data = join_data.reindex(columns = train_data.columns)\n",
    "\n",
    "print(\"The size of the train data set is now: {} entries and {} features.\".format(train_data.shape[0],\n",
    "                                                                                  train_data.shape[1]))\n",
    "\n",
    "train_data.to_csv('Dataset/train.csv', encoding='utf-8',index = None)\n",
    "print(\"Train Data saved as Dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the test data set is now: 13096 entries and 29 features.\n",
      "Test Data saved as Dataset/test.csv\n"
     ]
    }
   ],
   "source": [
    "# generate RUL\n",
    "rul = pd.DataFrame(test_data.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "test_data = test_data.merge(truth_df, on=['id'], how='left')\n",
    "test_data['RUL'] = test_data['max'] - test_data['cycle']\n",
    "test_data.drop('max', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# generate label columns w0 and w1 for test data\n",
    "test_data['label1'] = np.where(test_data['RUL'] <= w1, 1, 0 )\n",
    "\n",
    "# MinMax normalization (from 0 to 1)\n",
    "test_data['cycle_norm'] = test_data['cycle']\n",
    "norm_test_data = pd.DataFrame(min_max_scaler.transform(test_data[cols_normalize]),\n",
    "                              columns=cols_normalize, index=test_data.index)\n",
    "test_join_data = test_data[test_data.columns.difference(cols_normalize)].join(norm_test_data)\n",
    "test_data = test_join_data.reindex(columns = test_data.columns)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"The size of the test data set is now: {} entries and {} features.\".format(test_data.shape[0],\n",
    "                                                                                 test_data.shape[1]))\n",
    "\n",
    "test_data.to_csv('Dataset/test.csv', encoding='utf-8',index = None)\n",
    "print(\"Test Data saved as Dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neural Network is supervised classification that aim for accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
